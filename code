# Функция для очистки текста: удаление стоп-слов и лемматизация
def process_text(text):
    # Токенизация
    tokens = word_tokenize(text)

    # Удаление стоп-слов и лемматизация
    cleaned_tokens = [
        lemmatizer.lemmatize(word.lower()) for word in tokens
        if word.isalnum() and word.lower() not in stop_words  # Убираем стоп-слова и небуквенные токены
    ]

    return cleaned_tokens
